{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando Spark e CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "findspark.init()\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Criar a sessão Spark de forma mais simples\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestePySpark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "caminho_csv = \"dados/dados_gol.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leitura do CSV usando rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(caminho_csv)\n",
    "\n",
    "rdd_data = rdd.zipWithIndex().filter(lambda x: x[1] > 1).map(lambda x: x[0])\n",
    "\n",
    "rdd_cleaned = rdd_data.map(lambda line: [field.replace('\"', '') for field in line.split(\";\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando Cabeçalho e add no dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.option(\"header\", \"false\").csv(caminho_csv))\n",
    "# Obter a segunda linha como cabeçalho\n",
    "novo_header = df.collect()[1]  \n",
    "colunas = [str(c).replace('\"', '').strip() for c in novo_header[0].split(\";\")]\n",
    "df = rdd_cleaned.toDF(colunas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando o Filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+--------+\n",
      "|MES| ANO|    RPK| MERCADO|\n",
      "+---+----+-------+--------+\n",
      "|  1|2001| 887040|SBBHSBSP|\n",
      "|  1|2001| 743280|SBBHSBSV|\n",
      "|  1|2001|1172660|SBBRSBGL|\n",
      "|  1|2001|2845110|SBBRSBSP|\n",
      "|  1|2001|1144680|SBBRSBSV|\n",
      "|  1|2001|       |SBFLSBGL|\n",
      "|  1|2001|1188820|SBFLSBPA|\n",
      "|  1|2001|2937760|SBFLSBSP|\n",
      "|  1|2001|1960530|SBBRSBGL|\n",
      "|  1|2001|       |SBFLSBGL|\n",
      "|  1|2001|       |SBGLSBPA|\n",
      "|  1|2001|2606760|SBGLSBSP|\n",
      "|  1|2001| 756378|SBGLSBSV|\n",
      "|  1|2001|1264330|SBFLSBPA|\n",
      "|  1|2001|       |SBGLSBPA|\n",
      "|  1|2001|       |SBPASBSP|\n",
      "|  1|2001| 859320|SBBHSBSP|\n",
      "|  1|2001|2486300|SBBRSBSP|\n",
      "|  1|2001|2892860|SBFLSBSP|\n",
      "|  1|2001|3033360|SBGLSBSP|\n",
      "+---+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---+---+-------+\n",
      "|MES|ANO|RPK|MERCADO|\n",
      "+---+---+---+-------+\n",
      "|  0|  0|  0|      0|\n",
      "+---+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtro\n",
    "df_filtrado = df.filter(\n",
    "    (df[\"EMPRESA_SIGLA\"] == \"GLO\") &\n",
    "    (df[\"GRUPO_DE_VOO\"] == \"REGULAR\") &\n",
    "    (df[\"NATUREZA\"] == \"DOMÉSTICA\")\n",
    ")\n",
    "df_prep = df_filtrado.withColumn(\"MERCADO\",\n",
    "        concat(\n",
    "            F.least(F.col(\"AEROPORTO_DE_ORIGEM_SIGLA\"), F.col(\"AEROPORTO_DE_DESTINO_SIGLA\")),\n",
    "            F.greatest(F.col(\"AEROPORTO_DE_ORIGEM_SIGLA\"), F.col(\"AEROPORTO_DE_DESTINO_SIGLA\")))) \\\n",
    "    .select( \"MES\", \"ANO\", \"RPK\", \"MERCADO\") \\\n",
    "    .na.drop()\n",
    "\n",
    "df_final = df_prep.withColumn(\n",
    "    \"MES\",\n",
    "    F.lpad(df_prep[\"MES\"].cast(\"string\"), 2, '0') \n",
    ")\n",
    "\n",
    "df_prep.show()\n",
    "df_final.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df_final.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando Filtro e o campo Mercado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+--------+\n",
      "|MES|ANO |RPK    |MERCADO |\n",
      "+---+----+-------+--------+\n",
      "|01 |2001|887040 |SBBHSBSP|\n",
      "|01 |2001|743280 |SBBHSBSV|\n",
      "|01 |2001|1172660|SBBRSBGL|\n",
      "|01 |2001|2845110|SBBRSBSP|\n",
      "|01 |2001|1144680|SBBRSBSV|\n",
      "|01 |2001|       |SBFLSBGL|\n",
      "|01 |2001|1188820|SBFLSBPA|\n",
      "|01 |2001|2937760|SBFLSBSP|\n",
      "|01 |2001|1960530|SBBRSBGL|\n",
      "|01 |2001|       |SBFLSBGL|\n",
      "|01 |2001|       |SBGLSBPA|\n",
      "|01 |2001|2606760|SBGLSBSP|\n",
      "|01 |2001|756378 |SBGLSBSV|\n",
      "|01 |2001|1264330|SBFLSBPA|\n",
      "|01 |2001|       |SBGLSBPA|\n",
      "|01 |2001|       |SBPASBSP|\n",
      "|01 |2001|859320 |SBBHSBSP|\n",
      "|01 |2001|2486300|SBBRSBSP|\n",
      "|01 |2001|2892860|SBFLSBSP|\n",
      "|01 |2001|3033360|SBGLSBSP|\n",
      "+---+----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtrado = df.filter(\n",
    "        (df[\"EMPRESA_SIGLA\"] == \"GLO\") &\n",
    "        (df[\"GRUPO_DE_VOO\"] == \"REGULAR\") &\n",
    "        (df[\"NATUREZA\"] == \"DOMÉSTICA\")\n",
    "    )\n",
    "df_prep = df_filtrado.withColumn(\"MERCADO\",\n",
    "        concat(\n",
    "            F.least(F.col(\"AEROPORTO_DE_ORIGEM_SIGLA\"), F.col(\"AEROPORTO_DE_DESTINO_SIGLA\")),\n",
    "            F.greatest(F.col(\"AEROPORTO_DE_ORIGEM_SIGLA\"), F.col(\"AEROPORTO_DE_DESTINO_SIGLA\")))) \\\n",
    "    .select( \"MES\", \"ANO\", \"RPK\", \"MERCADO\")\n",
    "\n",
    "df_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DB = df_prep.withColumn(\"mes\", col(\"mes\").cast(\"int\")) \\\n",
    "             .withColumn(\"ano\", col(\"ano\").cast(\"int\")) \\\n",
    "             .withColumn(\"rpk\", col(\"rpk\").cast(\"double\"))\n",
    "\n",
    "df_DB = df_DB.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conectando com o PSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "jdbc_driver_path = \"C:/Users/User/AppData/Local/Programs/Python/Python311/Lib/site-packages/pyspark/jars/postgresql-42.7.5.jar\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgreSQL Connection\") \\\n",
    "    .config(\"spark.jars\", jdbc_driver_path) \\\n",
    "    .config(\"spark.driver.extraClassPath\", jdbc_driver_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "database_url = \"jdbc:postgresql://localhost:5432/anac\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escrevendo o PSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DB.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", database_url) \\\n",
    "    .option(\"dbtable\", \"voos\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"root\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lendo o Banco PSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+--------+---------+\n",
      "| id| ano|mes| mercado|      rpk|\n",
      "+---+----+---+--------+---------+\n",
      "|  1|2012|  6|SBILSBSV| 549900.0|\n",
      "|  2|2012|  6|SBIZSBBR|3975550.0|\n",
      "|  3|2012|  6|SBIZSBSL|1452580.0|\n",
      "|  4|2012|  6|SBJPSBAR|  40824.0|\n",
      "|  5|2012|  6|SBJPSBBR|6005780.0|\n",
      "|  6|2012|  6|SBJPSBGL|1.35814E7|\n",
      "|  7|2012|  6|SBJPSBGR|      0.0|\n",
      "|  8|2012|  6|SBJPSBSV|2562850.0|\n",
      "|  9|2012|  6|SBJUSBFZ| 868411.0|\n",
      "| 10|2012|  6|SBJUSBRF|1253910.0|\n",
      "| 11|2012|  6|SBJVSBKP|      0.0|\n",
      "| 12|2012|  6|SBJVSBSP|1388040.0|\n",
      "| 13|2012|  6|SBKGSBSV|3405680.0|\n",
      "| 14|2012|  6|SBKPSBBR|6066400.0|\n",
      "| 15|2012|  6|SBKPSBCF|4226530.0|\n",
      "| 16|2012|  6|SBKPSBCT|1379600.0|\n",
      "| 17|2012|  6|SBKPSBGL|5765030.0|\n",
      "| 18|2012|  6|SBKPSBGR|  38678.0|\n",
      "| 19|2012|  6|SBKPSBPA| 328624.0|\n",
      "| 20|2012|  6|SBKPSBSP|  21924.0|\n",
      "+---+----+---+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "# Lendo uma tabela\n",
    "df_DB = spark.read.jdbc(url=database_url, \n",
    "          table=\"voos\", \n",
    "          properties=properties)\n",
    "\n",
    "df_DB.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
